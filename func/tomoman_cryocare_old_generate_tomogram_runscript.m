function tomoman_cryocare_old_generate_tomogram_runscript(t,p)
%% will_novactf_generate_tomogram_runscript
% A function to generate a 'runscript' for running novaCTF on a tilt-stack.
% When run, the runscript first runs parallel processing of tilt-stacks via
% MPI; when the MPI job is completed, it finishes the tomogram by running
% novaCTF. The tomogram is then binned via Fourier cropping and the 
% intermediate files are deleted. 
%
% WW 01-2018

%% Initialize

%% Check for refined center (__FUTURE__: implement xaxis tilt based on motl)
% 
% if isfield(p,'mean_z')
%     tomo_idx = p.mean_z(1,:) == t.tomo_num; % Find tomogram index
%     mean_z = round(p.mean_z(2,tomo_idx));   % Parse mean Z value
%     cen_name = [t.stack_dir,'/imod_batchprocess/refined_cen.txt'];
%     dlmwrite(cen_name,mean_z);
%     new_cen = ['DefocusShiftFile ',cen_name];
% else
%     new_cen = [];
% end
%     


%% Generate run script

% Open run script
rscript = fopen([t.stack_dir,'/cryocare/run_cryocare.sh'],'w');

% link files
link_odd_cmd = ['ln -sf ' p.odd_tomodir '/' num2str(t.tomo_num) '.rec ' t.stack_dir '/cryocare/' num2str(t.tomo_num) '_odd.rec'];
link_evn_cmd = ['ln -sf ' p.evn_tomodir '/' num2str(t.tomo_num) '.rec ' t.stack_dir '/cryocare/' num2str(t.tomo_num) '_evn.rec'];

system(link_odd_cmd);
system(link_evn_cmd);

% run params 
path_to_tomo_folder = [t.stack_dir '/cryocare/'];
even_tomo_filename = [num2str(t.tomo_num) '_odd.rec'];
odd_tomo_filename = [num2str(t.tomo_num) '_evn.rec'];
box_size = num2str(p.cc_boxsize);
neural_net_depth = num2str(p.cc_depth);
model_name = [num2str(t.tomo_num) '_box' num2str(p.cc_boxsize) '_d' num2str(p.cc_depth)];
path_to_model_folder = [t.stack_dir '/cryocare/'];
skip_training = 'n';

% % copy cryocare CLI 
% cli_copy_cmd =  ['cp ' p.cryocare_cli '/* ' t.stack_dir '/cryocare/'];
% system(cli_copy_cmd);

% % Check whether to skip the training
% if p.cc_train
%     skip_training = 'n';
% else
%     skip_training = 'y';

switch p.queue
%     case 'p.512g'
%         error('Oops!! 404');
%         fprintf(rscript,['#! /usr/bin/env bash\n\n',...
%             '#$ -pe openmpi 40\n',...            % Number of cores
%             '#$ -l h_vmem=128G\n',...            % Memory limit
%             '#$ -l h_rt=604800\n',...              % Wall time
%             '#$ -q ',p.queue,'\n',...                       %  queue
%             '#$ -e ',t.stack_dir,'/novactf/error_novactf\n',...       % Error file
%             '#$ -o ',t.stack_dir,'/novactf/log_novactf\n',...         % Log file
%             '#$ -S /bin/bash\n',...                      % Submission environment
%             'source ~/.bashrc\n\n',]);                      % Get proper envionment; i.e. modules

%     case 'p.192g'
%         error('Oops!! 404');
%         fprintf(rscript,['#! /usr/bin/env bash\n\n',...
%             '#$ -pe openmpi 16\n',...            % Number of cores
%             '#$ -l h_vmem=128G\n',...            % Memory limit
%             '#$ -l h_rt=604800\n',...              % Wall time
%             '#$ -q ',p.queue,'\n',...                       %  queue
%             '#$ -e ',t.stack_dir,'/novactf/error_novactf\n',...       % Error file
%             '#$ -o ',t.stack_dir,'/novactf/log_novactf\n',...         % Log file
%             '#$ -S /bin/bash\n',...                      % Submission environment
%             'source ~/.bashrc\n\n',]);                      % Get proper envionment; i.e. modules        
%     case 'local'
%         fprintf(rscript,['#!/usr/bin/env bash \n\n','echo $HOSTNAME\n','set -e \n','set -o nounset \n\n']);
%             
%     case 'p.hpcl67'
%         fprintf(rscript,['#!/bin/bash -l\n',...
%             '# Standard output and error:\n',...
%             '#SBATCH -e ' ,t.stack_dir,'/imod_batchprocess/error_imod_batchprocess\n',...
%             '#SBATCH -o ' ,t.stack_dir,'/imod_batchprocess/log_imod_batchprocess\n',...
%             '# Initial working directory:\n',...
%             '#SBATCH -D ./\n',...
%             '# Job Name:\n',...
%             '#SBATCH -J IMOD\n',...
%             '# Queue (Partition):\n',...
%             '#SBATCH --partition=p.hpcl67 \n',...
%             '# Number of nodes and MPI tasks per node:\n',...
%             '#SBATCH --nodes=1\n',...
%             '#SBATCH --ntasks=40\n',...
%             '#SBATCH --ntasks-per-node=40\n',...
%             '#SBATCH --cpus-per-task=1\n',...            %'#SBATCH --gres=gpu:2\n',...
%             '#\n',...
%             '#SBATCH --mail-type=none\n',...
%             '#SBATCH --mem 510000\n',...
%             '#\n',...
%             '# Wall clock limit:\n',...
%             '#SBATCH --time=168:00:00\n',...
%             'echo "setting up environment"\n',...
%             'module purge\n',...
%             'module load intel/18.0.5\n',...
%             'module load impi/2018.4\n',...
%             '#load module for your application\n',...
%             'module load FOURIER3D/06-10-20\n',...
%             'module load IMOD/4.11.1\n',...
%             'export IMOD_PROCESSORS=40\n']);                      % Get proper envionment; i.e. modules
 
    case 'p.hpcl8'
        fprintf(rscript,['#!/bin/bash -l\n',...
            '# Standard output and error:\n',...
            '#SBATCH -e ' ,t.stack_dir,'/cryocare/error_cryocare\n',...
            '#SBATCH -o ' ,t.stack_dir,'/cryocare/log_cryocare\n',...
            '# Initial working directory:\n',...
            '#SBATCH -D ./\n',...
            '# Job Name:\n',...
            '#SBATCH -J Cryocare\n',...
            '# Queue (Partition):\n',...
            '#SBATCH --partition=p.hpcl8 \n',...
            '# Number of nodes and MPI tasks per node:\n',...
            '#SBATCH --nodes=1\n',...
            '#SBATCH --ntasks=1\n',...
            '#SBATCH --ntasks-per-node=1\n',...
            '#SBATCH --cpus-per-task=1\n',...
            '#SBATCH --gres=gpu:2\n',...
            '#\n',...
            '#SBATCH --mail-type=none\n',...
            '#SBATCH --mem 378880\n',...
            '#\n',...
            '# Wall clock limit:\n',...
            '#SBATCH --time=168:00:00\n',...
            'echo "setting up environment"\n',...
            'module purge\n',...
            'module load intel/18.0.5\n',...
            'module load impi/2018.4\n',...
            '#load module for your application\n',...
            'module load CRYO-CARE/310121\n']);                      % Get proper envionment; i.e. modules
        
    otherwise
            error('only "p.hpcl8" are supported queques for p.queue!!!!')
        
    
end

% Run parallel scripts
fprintf(rscript,['# Denoise tomogram using Cryocare CLI','\n']);
fprintf(rscript,['srun time python ' p.cryocare_cli '/03_Training_Data_Generation_Ricardo.py ' path_to_tomo_folder ' ' even_tomo_filename ' ' odd_tomo_filename ' ' box_size ' ' neural_net_depth ' ' model_name ' ' path_to_model_folder '\n']);

fprintf(rscript,['srun time python ' p.cryocare_cli '/04_Train_cryoCARE_Network_Ricardo.py ' path_to_tomo_folder ' ' box_size ' ' neural_net_depth ' ' model_name '\n']);
         
fprintf(rscript,['srun time python ' p.cryocare_cli '/05_Run_cryoCARE_Network_Ricardo.py ' path_to_tomo_folder ' ' even_tomo_filename ' ' odd_tomo_filename ' ' box_size ' ' neural_net_depth ' ' model_name ' ' path_to_model_folder '\n']);

fprintf(rscript,['mv ' t.stack_dir '/cryocare/cryocare_' num2str(p.cc_boxsize) '_d' num2str(p.cc_depth) '/cryocare_' num2str(p.cc_boxsize) '_d' num2str(p.cc_depth) '.rec ' p.cryocare_tomodir '/' num2str(t.tomo_num) '.rec']);
% Close file and make executable
fclose(rscript);
system(['chmod +x ',t.stack_dir,'/cryocare/run_cryocare.sh']);

                 


